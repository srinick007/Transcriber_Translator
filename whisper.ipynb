{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e811cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sridh\\AppData\\Local\\Temp\\ipykernel_28624\\897549616.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3a8cc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b07bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0-dev20220812\n"
     ]
    }
   ],
   "source": [
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb5be1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34af8987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8dc5af",
   "metadata": {},
   "source": [
    "# whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bcd2865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b076e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34bf490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4597cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"small\" , device=\"cuda\")\n",
    "result = model.transcribe(\"french.mp3\" , task=\"transcribe\")\n",
    "text=result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5beb853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bonjour dans notre coll√®ge il y a au moins 600 √©l√®ves et 50 professeurs pour 40 salles et 10 salles sp√©cifiques. Mon emploi du temps du lundi et je commence par fran√ßais √† 8h30 jusqu'√† 9h20 puis anglais de 9h26 √† 10h00 et ensuite de 10h35 √† 11h30 fran√ßais puis on mange 2h00. Apr√®s manger il y a sport soit gymnastique ou natation de 2h00 et on finit par maths qui commence √† 15h50 et finit √† 16h26. L'horaire d'une journ√©e de jupes est de 8h30 √† 16h26 Les mati√®res et mes pr√©f√©rences sont ma mati√®re pr√©f√©r√©e et le fran√ßais parce que c'est int√©ressant. Celle que j'aime le moins c'est Histoire G√©o je n'arrive pas √† suivre et voil√†. Les tenues des √©l√®ves sont ce qu'on veut. Il faut juste un peu respecter les r√®gles que les tenues ne soient pas vulgaires. Le contenu de mon sac √† dos d'une journ√©e normale sur le lundi. Alors j'ai 4 cahiers, 1 cahier de fran√ßais, 2 cahiers de maths et 1 cahier d'histoire. Je me trouve ma tablette, ma r√®gle et voil√†. Et mon carnet et mon agenda. Voil√†. Au revoir.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5357bead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "Life is a focus, life is a purpose. A purpose achieved by virtues which are developed with personal effort, do not gamble on life. It may not happen twice.\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(r\"C:\\Users\\sridh\\Desktop\\audio\\10sec.wav\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa95525a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ja'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(probs, key=probs.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "071eff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang={'ja':'jpn_Jpan',\n",
    "     'en':'eng_Latn'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e06a0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jpn_Jpan\n"
     ]
    }
   ],
   "source": [
    "if max(probs, key=probs.get) in lang:\n",
    "    print(lang[max(probs, key=probs.get)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec1161",
   "metadata": {},
   "source": [
    "# nllb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1407744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "fasttext.FastText.eprint = lambda x: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db96b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lang_model = \"lid218e.bin\" # Path of model file\n",
    "\n",
    "detect_model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "#text=\"hello there\"\n",
    "\n",
    "#predictions = detect_model.predict(text, k=1)\n",
    "\n",
    "#print(predictions) #(('__label__arb_Arab'), array([0.99960977]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348942b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_lang = predictions[0][0].replace('__label__', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e965f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'facebook/nllb-200-distilled-600M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68f3cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c8d6ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f2d03519e94dc2a8b9d2e799d95692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sridh\\anaconda3\\envs\\tf2.4_python3.9\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sridh\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ee21349a88433eaa167363eb560d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)neration_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dfd9dc3004498faf5cfd90de2a54a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f042ccecdc0244248055666e655ea1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)ncepiece.bpe.model\";:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62abe8cdfe2544af9971f16987e4fa1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)\"tokenizer.json\";:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaab48ce740e44938532644e95c78909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline('translation', model=translate_model, tokenizer=tokenizer, src_lang=input_lang, tgt_lang=\"jpn_Jpan\", max_length = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5a460816",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "„Åì„Çì„Å´„Å°„ÅØ,ÁßÅ„Åü„Å°„ÅÆÂ§ßÂ≠¶„Å´„ÅØÂ∞ë„Å™„Åè„Å®„ÇÇ600‰∫∫„ÅÆÁîüÂæí„Å®50‰∫∫„ÅÆÊïôÂ∏´„Åå40ÈÉ®Â±ã„Å®10„Å§„ÅÆÁâπÂÆö„ÅÆÈÉ®Â±ã„Å´„ÅÑ„Åæ„Åô.ÁßÅ„ÅÆÊó•Á®ã„ÅØÊúàÊõúÊó•„Åß„Åô. ÁßÅ„ÅØ8ÊôÇ30ÂàÜ„Åã„Çâ9ÊôÇ20ÂàÜ„Åæ„Åß„Éï„É©„É≥„ÇπË™û„Åã„ÇâÂßã„ÇÅ„Åæ„Åô. 9ÊôÇ26ÂàÜ„Åã„Çâ10ÊôÇ30ÂàÜ„Åæ„ÅßËã±Ë™û„Åß„Åô. „Åù„Çå„Åã„Çâ10ÊôÇ35ÂàÜ„Åã„Çâ11ÊôÇ30ÂàÜ„Åæ„Åß„Éï„É©„É≥„ÇπË™û„Åß„Åô. „Åù„Çå„Åã„Çâ2ÊôÇÈ£ü„Åπ„Åæ„Åô. È£ü„Åπ„ÅüÂæå„Å´„Çπ„Éù„Éº„ÉÑ„Åå„ÅÇ„Çä„Åæ„Åô. ÈÅãÂãï„ÇÑ2ÊôÇÊ≥≥„Åé„Åß„Åô. „Åù„Åó„Å¶ÊúàÊõúÊó•„Å´„ÅØÊï∞Â≠¶„ÅßÁµÇ„Çè„Çä„Åæ„Åô. 15ÊôÇ50ÂàÜ„Åã„Çâ16ÊôÇ26ÂàÜ„Åæ„Åß„Åß„Åô. Èù¥‰∏ã„ÅÆÊó•Á®ã„ÅØ8ÊôÇ30ÂàÜ„Åã„Çâ16ÊôÇ26ÂàÜ„Åß„Åô. „ÉÜ„Éº„Éû„Å®ÁßÅ„ÅÆÂ•Ω„Åø„ÅÆÂÜÖÂÆπ„ÅØÁßÅ„ÅÆÂ•Ω„Åø„ÅÆÂÜÖÂÆπ„Åß„Åô. „Åù„Åó„Å¶„Éï„É©„É≥„ÇπË™û„ÅØ,„Åù„Çå„ÅåÈù¢ÁôΩ„ÅÑ„Åã„Çâ„Åß„Åô. ÁßÅ„Åå‰∏ÄÁï™Â•Ω„Åç„Å™„ÅÆ„ÅØ,Ê≠¥Âè≤„Åß„Åô. ÁßÅ„ÅØÂà∞ÁùÄ„Åó,„Åù„Çå„ÇíÂÆà„Å£„Å¶„ÅÑ„Åæ„Åô. „Åï„Å¶,ÁîüÂæí„Åü„Å°„ÅØ15ÊôÇ50ÂàÜ„Åã„Çâ16ÊôÇ00ÂàÜ„Åæ„Åß,Êï∞Â≠¶‰∏ä„ÅÆË¶èÂâá„ÇíÂ∞äÈáç„Åó„Åæ„Åô. „Åù„Åó„Å¶,ÁßÅ„ÅÆ„Çπ„Ç±„Ç∏„É•„Éº„É´1Êó•ÁõÆ„Åß„Åô. ÁßÅ„ÅØÁßÅ„ÅÆ„Çπ„Ç±„Ç∏„É•„Éº„É´1„Å´,ÁßÅ„ÅÆ„Çπ„Ç±„Ç∏„É•„Éº„É´1Êó•ÁõÆ,„Åù„Åó„Å¶ÁßÅ„ÅÆ„Çπ„Ç±„Ç∏„É•„Éº„É´1„Å´,ÁßÅ„ÅÆ„Çπ„Ç±„Ç∏„É•„Éº„É´1„ÅÆË¶èÂâá„ÇíÈÅµÂÆà„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô.\n"
     ]
    }
   ],
   "source": [
    "output = translator(text)\n",
    "translated_text = output[0]['translation_text']\n",
    "print(translated_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26db18",
   "metadata": {},
   "source": [
    "# building gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d71aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b8918e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"base\" , device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4780b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text,out_lang):\n",
    "    predictions = detect_model.predict(text, k=1)\n",
    "    \n",
    "    input_lang = predictions[0][0].replace('__label__', '')\n",
    "    \n",
    "    translator = pipeline('translation', model=translate_model, tokenizer=tokenizer, src_lang=input_lang, tgt_lang=out_lang, max_length = 400)\n",
    "    \n",
    "    output = translator(text)\n",
    "    \n",
    "    translated_text = output[0]['translation_text']\n",
    "    \n",
    "    return translated_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transcribe(audio,out_lang):\n",
    "    #whisper\n",
    "    audio = whisper.load_audio(audio)\n",
    "    result = model.transcribe(audio , task=\"transcribe\")\n",
    "    translated=translate(result[\"text\"],out_lang)\n",
    "    return result[\"text\"],translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7194d392",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sridh\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\inputs.py:319: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\sridh\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sridh\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\processing_utils.py:241: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    title = 'translator (not so fast oneüòÇ)', \n",
    "    fn=transcribe, \n",
    "    inputs=[\n",
    "        gr.inputs.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "        gr.Dropdown([\"eng_Latn\",\"jpn_Jpan\", \"spa_Latn\", \"fra_Latn\"]),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"text\"),\n",
    "        gr.Textbox(label=\"translated\"),\n",
    "\n",
    "    ]).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd9b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
